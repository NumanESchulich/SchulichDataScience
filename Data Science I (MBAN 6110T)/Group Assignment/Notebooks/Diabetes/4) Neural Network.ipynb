{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Neural Network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Import TensorFlow for Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Read the data file\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/NumanESchulich/SchulichDataScience/main/Data%20Science%20I%20(MBAN%206110T)/Group%20Assignment/Datasets/Full%20Dataset%20(RAW).csv\")\n",
    "\n",
    "# Reversing some Binary Columns (Changing column names and reversing values for specific columns)\n",
    "columns_to_reverse = {\n",
    "    'DiffWalk': 'NoDiffWalk',\n",
    "    'HighBP': 'NoHighBP',\n",
    "    'HighChol': 'NoHighChol',\n",
    "    'HeartDiseaseorAttack': 'NoHeartDiseaseorAttack',\n",
    "    'Stroke': 'NoStroke',\n",
    "    'Smoker': 'NoSmoker',\n",
    "    'HvyAlcoholConsump': 'NoHvyAlcoholConsump'\n",
    "}\n",
    "\n",
    "for old_col, new_col in columns_to_reverse.items():\n",
    "    df[new_col] = 1 - df[old_col]\n",
    "    df.drop(columns=[old_col], inplace=True)\n",
    "\n",
    "# Scaling adjustments\n",
    "\n",
    "# PhysHlth scaling\n",
    "def phys_ment_hlth_scale(days):\n",
    "    if 1 <= days <= 6:\n",
    "        return 1\n",
    "    elif 7 <= days <= 12:\n",
    "        return 0.75\n",
    "    elif 13 <= days <= 18:\n",
    "        return 0.5\n",
    "    elif 19 <= days <= 24:\n",
    "        return 0.25\n",
    "    elif 25 <= days <= 30:\n",
    "        return 0\n",
    "    return days\n",
    "\n",
    "df['PhysHlth'] = df['PhysHlth'].apply(phys_ment_hlth_scale)\n",
    "\n",
    "# MentHlth scaling\n",
    "df['MentHlth'] = df['MentHlth'].apply(phys_ment_hlth_scale)\n",
    "\n",
    "# Adding the NotObese column\n",
    "df['NotObese'] = df['BMI'].apply(lambda x: 1 if x < 30 else 0)\n",
    "\n",
    "# Age scaling\n",
    "age_scale = {1: 1, 2: 1, 3: 1, 4: 0.75, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.5, 10: 0.25, 11: 0, 12: 0, 13: 0}\n",
    "df['Age'] = df['Age'].map(age_scale)\n",
    "\n",
    "# Feature Engineered Columns\n",
    "df['PhysicalCondition'] = (df['GenHlth'] + df['PhysHlth']) / 2\n",
    "df['NoDisease'] = (df['NoHighBP'] + df['NoHighChol']) / 2\n",
    "df['Lifestyle'] = (df['NoSmoker'] + df['Fruits']) / 2\n",
    "\n",
    "# Splitting the dataset into features and target variable\n",
    "X = df[['Age', 'MentHlth', 'NotObese', 'NoDocbcCost', 'PhysicalCondition', 'NoDisease', 'PhysHlth', 'Lifestyle', 'GenHlth', 'Income', 'Fruits']]\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# Split off 20% of the data for validation later on\n",
    "X_temp, X_val, y_temp, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_temp = scaler.fit_transform(X_temp)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Applying SMOTE to the remaining 80% of the data to oversample the minority class significantly\n",
    "smote = SMOTE(sampling_strategy=1.0, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_temp, y_temp)\n",
    "\n",
    "# Splitting the resampled dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Neural Network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_dim = X_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Neural Network model\n",
    "history = nn_model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                       epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = nn_model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predictions and evaluation metrics\n",
    "y_val_pred = (nn_model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Epoch 1/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5263 - accuracy: 0.7370 - val_loss: 0.5244 - val_accuracy: 0.7026\n",
      "Epoch 2/100\n",
      "8730/8730 [==============================] - 16s 2ms/step - loss: 0.5220 - accuracy: 0.7397 - val_loss: 0.5318 - val_accuracy: 0.6980\n",
      "Epoch 3/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5208 - accuracy: 0.7399 - val_loss: 0.5351 - val_accuracy: 0.7135\n",
      "Epoch 4/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5204 - accuracy: 0.7403 - val_loss: 0.5169 - val_accuracy: 0.7269\n",
      "Epoch 5/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5198 - accuracy: 0.7408 - val_loss: 0.5237 - val_accuracy: 0.7187\n",
      "Epoch 6/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5195 - accuracy: 0.7404 - val_loss: 0.5429 - val_accuracy: 0.7061\n",
      "Epoch 7/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5190 - accuracy: 0.7411 - val_loss: 0.5335 - val_accuracy: 0.7127\n",
      "Epoch 8/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5188 - accuracy: 0.7409 - val_loss: 0.5225 - val_accuracy: 0.7021\n",
      "Epoch 9/100\n",
      "8730/8730 [==============================] - 16s 2ms/step - loss: 0.5184 - accuracy: 0.7415 - val_loss: 0.5489 - val_accuracy: 0.6994\n",
      "1586/1586 [==============================] - 2s 1ms/step - loss: 0.5169 - accuracy: 0.7269\n",
      "Validation Loss: 0.516944169998169\n",
      "Validation Accuracy: 0.7269394397735596\n",
      "1586/1586 [==============================] - 1s 707us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82     43739\n",
      "           1       0.31      0.78      0.44      6997\n",
      "\n",
      "    accuracy                           0.73     50736\n",
      "   macro avg       0.63      0.75      0.63     50736\n",
      "weighted avg       0.86      0.73      0.77     50736\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31446 12293]\n",
      " [ 1561  5436]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Neural Network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Import TensorFlow for Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Read the data file\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/NumanESchulich/SchulichDataScience/main/Data%20Science%20I%20(MBAN%206110T)/Group%20Assignment/Datasets/Full%20Dataset%20(RAW).csv\")\n",
    "\n",
    "# Reversing some Binary Columns (Changing column names and reversing values for specific columns)\n",
    "columns_to_reverse = {\n",
    "    'DiffWalk': 'NoDiffWalk',\n",
    "    'HighBP': 'NoHighBP',\n",
    "    'HighChol': 'NoHighChol',\n",
    "    'HeartDiseaseorAttack': 'NoHeartDiseaseorAttack',\n",
    "    'Stroke': 'NoStroke',\n",
    "    'Smoker': 'NoSmoker',\n",
    "    'HvyAlcoholConsump': 'NoHvyAlcoholConsump'\n",
    "}\n",
    "\n",
    "for old_col, new_col in columns_to_reverse.items():\n",
    "    df[new_col] = 1 - df[old_col]\n",
    "    df.drop(columns=[old_col], inplace=True)\n",
    "\n",
    "# Scaling adjustments\n",
    "\n",
    "# PhysHlth scaling\n",
    "def phys_ment_hlth_scale(days):\n",
    "    if 1 <= days <= 6:\n",
    "        return 1\n",
    "    elif 7 <= days <= 12:\n",
    "        return 0.75\n",
    "    elif 13 <= days <= 18:\n",
    "        return 0.5\n",
    "    elif 19 <= days <= 24:\n",
    "        return 0.25\n",
    "    elif 25 <= days <= 30:\n",
    "        return 0\n",
    "    return days\n",
    "\n",
    "df['PhysHlth'] = df['PhysHlth'].apply(phys_ment_hlth_scale)\n",
    "\n",
    "# MentHlth scaling\n",
    "df['MentHlth'] = df['MentHlth'].apply(phys_ment_hlth_scale)\n",
    "\n",
    "# Adding the NotObese column\n",
    "df['NotObese'] = df['BMI'].apply(lambda x: 1 if x < 30 else 0)\n",
    "\n",
    "# Age scaling\n",
    "age_scale = {1: 1, 2: 1, 3: 1, 4: 0.75, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.5, 10: 0.25, 11: 0, 12: 0, 13: 0}\n",
    "df['Age'] = df['Age'].map(age_scale)\n",
    "\n",
    "# Feature Engineered Columns\n",
    "df['PhysicalCondition'] = (df['GenHlth'] + df['PhysHlth']) / 2\n",
    "df['NoDisease'] = (df['NoHighBP'] + df['NoHighChol']) / 2\n",
    "df['Lifestyle'] = (df['NoSmoker'] + df['Fruits']) / 2\n",
    "\n",
    "# Splitting the dataset into features and target variable\n",
    "X = df[['Age', 'MentHlth', 'NotObese', 'NoDocbcCost', 'PhysicalCondition', 'NoDisease', 'PhysHlth', 'Lifestyle', 'GenHlth', 'Income', 'Fruits']]\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# Split off 20% of the data for validation later on\n",
    "X_temp, X_val, y_temp, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_temp = scaler.fit_transform(X_temp)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Applying SMOTE to the remaining 80% of the data to oversample the minority class significantly\n",
    "smote = SMOTE(sampling_strategy=1.0, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_temp, y_temp)\n",
    "\n",
    "# Splitting the resampled dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Neural Network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_dim = X_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Neural Network model\n",
    "history = nn_model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                       epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = nn_model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predictions and evaluation metrics\n",
    "y_val_pred = (nn_model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Model vs Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8730/8730 [==============================] - 18s 2ms/step - loss: 0.5007 - accuracy: 0.7562 - val_loss: 0.5192 - val_accuracy: 0.7316\n",
      "Epoch 2/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.4911 - accuracy: 0.7614 - val_loss: 0.5031 - val_accuracy: 0.7397\n",
      "Epoch 3/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.4884 - accuracy: 0.7631 - val_loss: 0.5309 - val_accuracy: 0.7194\n",
      "Epoch 4/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.4864 - accuracy: 0.7638 - val_loss: 0.5284 - val_accuracy: 0.7169\n",
      "Epoch 5/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.4851 - accuracy: 0.7646 - val_loss: 0.5448 - val_accuracy: 0.7232\n",
      "Epoch 6/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.4837 - accuracy: 0.7655 - val_loss: 0.5467 - val_accuracy: 0.7109\n",
      "Epoch 7/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.4826 - accuracy: 0.7662 - val_loss: 0.5366 - val_accuracy: 0.7144\n",
      "1586/1586 [==============================] - 2s 1ms/step - loss: 0.5031 - accuracy: 0.7397\n",
      "Validation Loss: 0.5030717253684998\n",
      "Validation Accuracy: 0.7396917343139648\n",
      "1586/1586 [==============================] - 1s 577us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.73      0.83     43739\n",
      "           1       0.32      0.77      0.45      6997\n",
      "\n",
      "    accuracy                           0.74     50736\n",
      "   macro avg       0.64      0.75      0.64     50736\n",
      "weighted avg       0.87      0.74      0.78     50736\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32120 11619]\n",
      " [ 1588  5409]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Neural Network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Import TensorFlow for Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Read the data file\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/NumanESchulich/SchulichDataScience/main/Data%20Science%20I%20(MBAN%206110T)/Group%20Assignment/Datasets/Full%20Dataset%20(RAW).csv\")\n",
    "\n",
    "# Dropping the ID column\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "# Splitting the dataset into features and target variable\n",
    "X = df.drop(columns=['Diabetes_binary'])\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# Split off 20% of the data for validation later on\n",
    "X_temp, X_val, y_temp, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_temp = scaler.fit_transform(X_temp)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Applying SMOTE to the remaining 80% of the data to oversample the minority class significantly\n",
    "smote = SMOTE(sampling_strategy=1.0, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_temp, y_temp)\n",
    "\n",
    "# Splitting the resampled dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Neural Network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_dim = X_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Neural Network model\n",
    "history = nn_model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                       epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = nn_model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predictions and evaluation metrics\n",
    "y_val_pred = (nn_model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model vs Raw Data and no SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3250 - accuracy: 0.8621 - val_loss: 0.3092 - val_accuracy: 0.8669\n",
      "Epoch 2/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3181 - accuracy: 0.8641 - val_loss: 0.3108 - val_accuracy: 0.8680\n",
      "Epoch 3/100\n",
      "5074/5074 [==============================] - 10s 2ms/step - loss: 0.3171 - accuracy: 0.8647 - val_loss: 0.3097 - val_accuracy: 0.8668\n",
      "Epoch 4/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3166 - accuracy: 0.8652 - val_loss: 0.3098 - val_accuracy: 0.8674\n",
      "Epoch 5/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3157 - accuracy: 0.8652 - val_loss: 0.3090 - val_accuracy: 0.8657\n",
      "Epoch 6/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3155 - accuracy: 0.8651 - val_loss: 0.3091 - val_accuracy: 0.8671\n",
      "Epoch 7/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3153 - accuracy: 0.8655 - val_loss: 0.3100 - val_accuracy: 0.8662\n",
      "Epoch 8/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3149 - accuracy: 0.8657 - val_loss: 0.3104 - val_accuracy: 0.8659\n",
      "Epoch 9/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3148 - accuracy: 0.8656 - val_loss: 0.3086 - val_accuracy: 0.8681\n",
      "Epoch 10/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3144 - accuracy: 0.8660 - val_loss: 0.3088 - val_accuracy: 0.8668\n",
      "Epoch 11/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3142 - accuracy: 0.8658 - val_loss: 0.3100 - val_accuracy: 0.8671\n",
      "Epoch 12/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3142 - accuracy: 0.8663 - val_loss: 0.3103 - val_accuracy: 0.8666\n",
      "Epoch 13/100\n",
      "5074/5074 [==============================] - 10s 2ms/step - loss: 0.3143 - accuracy: 0.8662 - val_loss: 0.3093 - val_accuracy: 0.8641\n",
      "Epoch 14/100\n",
      "5074/5074 [==============================] - 11s 2ms/step - loss: 0.3140 - accuracy: 0.8664 - val_loss: 0.3093 - val_accuracy: 0.8670\n",
      "1586/1586 [==============================] - 2s 1ms/step - loss: 0.3086 - accuracy: 0.8681\n",
      "Validation Loss: 0.3086070716381073\n",
      "Validation Accuracy: 0.8680621385574341\n",
      "1586/1586 [==============================] - 1s 710us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93     43739\n",
      "           1       0.58      0.15      0.24      6997\n",
      "\n",
      "    accuracy                           0.87     50736\n",
      "   macro avg       0.73      0.57      0.59     50736\n",
      "weighted avg       0.84      0.87      0.83     50736\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42966   773]\n",
      " [ 5921  1076]]\n",
      "1269/1269 [==============================] - 2s 1ms/step - loss: 0.3126 - accuracy: 0.8672\n",
      "Test Loss: 0.31263267993927\n",
      "Test Accuracy: 0.8671561479568481\n",
      "1269/1269 [==============================] - 1s 714us/step\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93     34929\n",
      "           1       0.60      0.14      0.23      5660\n",
      "\n",
      "    accuracy                           0.87     40589\n",
      "   macro avg       0.74      0.56      0.58     40589\n",
      "weighted avg       0.84      0.87      0.83     40589\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[34381   548]\n",
      " [ 4844   816]]\n",
      "1586/1586 [==============================] - 1s 630us/step\n",
      "Validation ROC-AUC Score: 0.8341841659574961\n",
      "1269/1269 [==============================] - 1s 618us/step\n",
      "Test ROC-AUC Score: 0.8294555275026867\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Neural Network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Import TensorFlow for Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Read the data file\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/NumanESchulich/SchulichDataScience/main/Data%20Science%20I%20(MBAN%206110T)/Group%20Assignment/Datasets/Full%20Dataset%20(RAW).csv\")\n",
    "\n",
    "# Dropping the ID column\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "# Splitting the dataset into features and target variable\n",
    "X = df.drop(columns=['Diabetes_binary'])\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# Split off 20% of the data for validation later on\n",
    "X_temp, X_val, y_temp, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_temp = scaler.fit_transform(X_temp)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Splitting the remaining 80% of the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Neural Network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_dim = X_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Neural Network model\n",
    "history = nn_model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                       epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = nn_model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predictions and evaluation metrics\n",
    "y_val_pred = (nn_model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = nn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predictions and evaluation metrics for test set\n",
    "y_test_pred = (nn_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# ROC-AUC score\n",
    "y_val_proba = nn_model.predict(X_val)\n",
    "roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC-AUC Score: {roc_auc}\")\n",
    "\n",
    "y_test_proba = nn_model.predict(X_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC-AUC Score: {roc_auc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8730/8730 [==============================] - 18s 2ms/step - loss: 0.5167 - accuracy: 0.7466 - val_loss: 0.5053 - val_accuracy: 0.7508\n",
      "Epoch 2/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5093 - accuracy: 0.7504 - val_loss: 0.5036 - val_accuracy: 0.7524\n",
      "Epoch 3/100\n",
      "8730/8730 [==============================] - 18s 2ms/step - loss: 0.5072 - accuracy: 0.7521 - val_loss: 0.5040 - val_accuracy: 0.7533\n",
      "Epoch 4/100\n",
      "8730/8730 [==============================] - 18s 2ms/step - loss: 0.5058 - accuracy: 0.7526 - val_loss: 0.5012 - val_accuracy: 0.7531\n",
      "Epoch 5/100\n",
      "8730/8730 [==============================] - 18s 2ms/step - loss: 0.5051 - accuracy: 0.7524 - val_loss: 0.5041 - val_accuracy: 0.7532\n",
      "Epoch 6/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5042 - accuracy: 0.7530 - val_loss: 0.5047 - val_accuracy: 0.7533\n",
      "Epoch 7/100\n",
      "8730/8730 [==============================] - 17s 2ms/step - loss: 0.5031 - accuracy: 0.7539 - val_loss: 0.5046 - val_accuracy: 0.7538\n",
      "Epoch 8/100\n",
      "8730/8730 [==============================] - 18s 2ms/step - loss: 0.5029 - accuracy: 0.7539 - val_loss: 0.5036 - val_accuracy: 0.7554\n",
      "Epoch 9/100\n",
      "8730/8730 [==============================] - 18s 2ms/step - loss: 0.5021 - accuracy: 0.7543 - val_loss: 0.5023 - val_accuracy: 0.7552\n",
      "2183/2183 [==============================] - 2s 1ms/step - loss: 0.5012 - accuracy: 0.7531\n",
      "Validation Loss: 0.5011608600616455\n",
      "Validation Accuracy: 0.753085732460022\n",
      "2183/2183 [==============================] - 1s 584us/step\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74     35002\n",
      "           1       0.73      0.81      0.76     34836\n",
      "\n",
      "    accuracy                           0.75     69838\n",
      "   macro avg       0.76      0.75      0.75     69838\n",
      "weighted avg       0.76      0.75      0.75     69838\n",
      "\n",
      "Validation Confusion Matrix:\n",
      "[[24528 10474]\n",
      " [ 6770 28066]]\n",
      "2183/2183 [==============================] - 1s 674us/step\n",
      "Validation ROC-AUC Score: 0.830986121938645\n",
      "1586/1586 [==============================] - 2s 1ms/step - loss: 0.5255 - accuracy: 0.7160\n",
      "Test Loss: 0.5254552960395813\n",
      "Test Accuracy: 0.7159610390663147\n",
      "1586/1586 [==============================] - 1s 622us/step\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81     43739\n",
      "           1       0.30      0.81      0.44      6997\n",
      "\n",
      "    accuracy                           0.72     50736\n",
      "   macro avg       0.63      0.76      0.63     50736\n",
      "weighted avg       0.87      0.72      0.76     50736\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[30643 13096]\n",
      " [ 1315  5682]]\n",
      "1586/1586 [==============================] - 1s 598us/step\n",
      "Test ROC-AUC Score: 0.8342263056283397\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Neural Network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Import TensorFlow for Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Read the data file\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/NumanESchulich/SchulichDataScience/main/Data%20Science%20I%20(MBAN%206110T)/Group%20Assignment/Datasets/Full%20Dataset%20(RAW).csv\")\n",
    "\n",
    "# Dropping the ID column\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "# Splitting the dataset into features and target variable\n",
    "X = df.drop(columns=['Diabetes_binary'])\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# Split off 20% of the data for the final test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the remaining data\n",
    "df_train_val = pd.concat([X_train_val, y_train_val], axis=1)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_train_val[df_train_val.Diabetes_binary == 0]\n",
    "df_minority = df_train_val[df_train_val.Diabetes_binary == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=42)  # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Split the upsampled data into features and target variable\n",
    "X_upsampled = df_upsampled.drop(columns=['Diabetes_binary'])\n",
    "y_upsampled = df_upsampled['Diabetes_binary']\n",
    "\n",
    "# Split the upsampled data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_upsampled, y_upsampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the Neural Network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_dim = X_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Neural Network model\n",
    "history = nn_model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                       epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = nn_model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predictions and evaluation metrics on validation set\n",
    "y_val_pred = (nn_model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "# ROC-AUC score for validation set\n",
    "y_val_proba = nn_model.predict(X_val)\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC-AUC Score: {roc_auc_val}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = nn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predictions and evaluation metrics on test set\n",
    "y_test_pred = (nn_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# ROC-AUC score for test set\n",
    "y_test_proba = nn_model.predict(X_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC-AUC Score: {roc_auc_test}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
